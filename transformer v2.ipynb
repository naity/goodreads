{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeb0cdfb-7baf-429b-ba31-f41bc5f0536e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fbabe8cdfb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from accelerate.utils import set_seed\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, \\\n",
    "    TrainingArguments, Trainer, get_scheduler\n",
    "\n",
    "np.random.seed(0) \n",
    "torch.manual_seed(0)\n",
    "\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46c69ab5-37b2-4041-8a9f-1b187efaef3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "      <th>date_added</th>\n",
       "      <th>date_updated</th>\n",
       "      <th>read_at</th>\n",
       "      <th>started_at</th>\n",
       "      <th>n_votes</th>\n",
       "      <th>n_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>18245960</td>\n",
       "      <td>dfdbb7b0eb5a7e4c26d59a937e2e5feb</td>\n",
       "      <td>5</td>\n",
       "      <td>This is a special book. It started slow for ab...</td>\n",
       "      <td>Sun Jul 30 07:44:10 -0700 2017</td>\n",
       "      <td>Wed Aug 30 00:00:26 -0700 2017</td>\n",
       "      <td>Sat Aug 26 12:05:52 -0700 2017</td>\n",
       "      <td>Tue Aug 15 13:23:18 -0700 2017</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>16981</td>\n",
       "      <td>a5d2c3628987712d0e05c4f90798eb67</td>\n",
       "      <td>3</td>\n",
       "      <td>Recommended by Don Katz. Avail for free in Dec...</td>\n",
       "      <td>Mon Dec 05 10:46:44 -0800 2016</td>\n",
       "      <td>Wed Mar 22 11:37:04 -0700 2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>28684704</td>\n",
       "      <td>2ede853b14dc4583f96cf5d120af636f</td>\n",
       "      <td>3</td>\n",
       "      <td>A fun, fast paced science fiction thriller. I ...</td>\n",
       "      <td>Tue Nov 15 11:29:22 -0800 2016</td>\n",
       "      <td>Mon Mar 20 23:40:27 -0700 2017</td>\n",
       "      <td>Sat Mar 18 23:22:42 -0700 2017</td>\n",
       "      <td>Fri Mar 17 23:45:40 -0700 2017</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>27161156</td>\n",
       "      <td>ced5675e55cd9d38a524743f5c40996e</td>\n",
       "      <td>0</td>\n",
       "      <td>Recommended reading to understand what is goin...</td>\n",
       "      <td>Wed Nov 09 17:37:04 -0800 2016</td>\n",
       "      <td>Wed Nov 09 17:38:20 -0800 2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>25884323</td>\n",
       "      <td>332732725863131279a8e345b63ac33e</td>\n",
       "      <td>4</td>\n",
       "      <td>I really enjoyed this book, and there is a lot...</td>\n",
       "      <td>Mon Apr 25 09:31:23 -0700 2016</td>\n",
       "      <td>Mon Apr 25 09:31:23 -0700 2016</td>\n",
       "      <td>Sun Jun 26 00:00:00 -0700 2016</td>\n",
       "      <td>Sat May 28 00:00:00 -0700 2016</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            user_id   book_id  \\\n",
       "0  8842281e1d1347389f2ab93d60773d4d  18245960   \n",
       "1  8842281e1d1347389f2ab93d60773d4d     16981   \n",
       "2  8842281e1d1347389f2ab93d60773d4d  28684704   \n",
       "3  8842281e1d1347389f2ab93d60773d4d  27161156   \n",
       "4  8842281e1d1347389f2ab93d60773d4d  25884323   \n",
       "\n",
       "                          review_id  rating  \\\n",
       "0  dfdbb7b0eb5a7e4c26d59a937e2e5feb       5   \n",
       "1  a5d2c3628987712d0e05c4f90798eb67       3   \n",
       "2  2ede853b14dc4583f96cf5d120af636f       3   \n",
       "3  ced5675e55cd9d38a524743f5c40996e       0   \n",
       "4  332732725863131279a8e345b63ac33e       4   \n",
       "\n",
       "                                         review_text  \\\n",
       "0  This is a special book. It started slow for ab...   \n",
       "1  Recommended by Don Katz. Avail for free in Dec...   \n",
       "2  A fun, fast paced science fiction thriller. I ...   \n",
       "3  Recommended reading to understand what is goin...   \n",
       "4  I really enjoyed this book, and there is a lot...   \n",
       "\n",
       "                       date_added                    date_updated  \\\n",
       "0  Sun Jul 30 07:44:10 -0700 2017  Wed Aug 30 00:00:26 -0700 2017   \n",
       "1  Mon Dec 05 10:46:44 -0800 2016  Wed Mar 22 11:37:04 -0700 2017   \n",
       "2  Tue Nov 15 11:29:22 -0800 2016  Mon Mar 20 23:40:27 -0700 2017   \n",
       "3  Wed Nov 09 17:37:04 -0800 2016  Wed Nov 09 17:38:20 -0800 2016   \n",
       "4  Mon Apr 25 09:31:23 -0700 2016  Mon Apr 25 09:31:23 -0700 2016   \n",
       "\n",
       "                          read_at                      started_at  n_votes  \\\n",
       "0  Sat Aug 26 12:05:52 -0700 2017  Tue Aug 15 13:23:18 -0700 2017       28   \n",
       "1                             NaN                             NaN        1   \n",
       "2  Sat Mar 18 23:22:42 -0700 2017  Fri Mar 17 23:45:40 -0700 2017       22   \n",
       "3                             NaN                             NaN        5   \n",
       "4  Sun Jun 26 00:00:00 -0700 2016  Sat May 28 00:00:00 -0700 2016        9   \n",
       "\n",
       "   n_comments  \n",
       "0           1  \n",
       "1           0  \n",
       "2           0  \n",
       "3           1  \n",
       "4           1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/goodreads_train.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05ba696b-d150-47c9-9926-3d7b4cd3f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train.loc[:, [\"review_text\", \"rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a2da146-8ebd-4fe4-b338-c544f4e81816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(810000, 2) (90000, 2)\n"
     ]
    }
   ],
   "source": [
    "# create a validation set\n",
    "train_df, valid_df = train_test_split(data, test_size=0.1, random_state=0)\n",
    "print(train_df.shape, valid_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2138cd83-9ef6-4d51-aefc-8cb4823ae04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "def get_dataloaders(batch_size):\n",
    "    train_ds = Dataset.from_pandas(train_df)\n",
    "    valid_ds = Dataset.from_pandas(valid_df)\n",
    "    \n",
    "    dataset = DatasetDict()\n",
    "    dataset[\"train\"] = train_ds\n",
    "    dataset[\"valid\"] = valid_ds\n",
    "    \n",
    "    dataset = dataset.rename_columns({\"rating\": \"label\"})\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.save_pretrained(\"models/tokenizer/\")\n",
    "    \n",
    "    def tokenize_func(dataset, col=\"review_text\"):\n",
    "        return tokenizer(dataset[col], padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    tokenized_datasets = dataset.map(tokenize_func, batched=True)\n",
    "    \n",
    "    tokenized_datasets = tokenized_datasets.remove_columns(\n",
    "        [\"review_text\", \"__index_level_0__\"])\n",
    "    tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "    tokenized_datasets.set_format(\"torch\")\n",
    "    \n",
    "    train_dl = DataLoader(tokenized_datasets[\"train\"],\n",
    "                      shuffle=True,\n",
    "                      batch_size=batch_size)\n",
    "    valid_dl = DataLoader(tokenized_datasets[\"valid\"],\n",
    "                          batch_size=batch_size)\n",
    "    return train_dl, valid_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7df7cb1-1583-45d5-921f-910096b14d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=6)\n",
    "\n",
    "# metric = evaluate.load(\"accuracy\")\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9f185b7-39eb-43d8-9be1-f8665eac6e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(batch_size=32, lr=5e-5, num_epochs=3,\n",
    "                  mixed_precision=\"fp16\", seed=0):\n",
    "    set_seed(seed)\n",
    "    accelerator = Accelerator(mixed_precision=mixed_precision)\n",
    "    model = get_model()\n",
    "    train_dl, valid_dl = get_dataloaders(batch_size)\n",
    "    \n",
    "    num_training_steps = num_epochs * len(train_dl)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps)\n",
    "    \n",
    "    model, optimizer, train_dl, valid_dl, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dl, valid_dl, lr_scheduler)\n",
    "    \n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch in train_dl:\n",
    "            # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            # loss.backward()\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        model.eval()\n",
    "        accurate = 0\n",
    "        num_elems = 0\n",
    "        for batch in valid_dl:\n",
    "            #batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            accurate_preds = accelerator.gather(predictions) == accelerator.gather(batch[\"labels\"])\n",
    "            num_elems += accurate_preds.shape[0]\n",
    "            accurate += accurate_preds.long().sum()\n",
    "            \n",
    "        accuracy = accurate.item() / num_elems\n",
    "        accelerator.print(f\"Epoch {epoch+1} accuracy: {100*accuracy:.2f}%\")\n",
    "        \n",
    "    # save model\n",
    "    accelerator.wait_for_everyone()\n",
    "    \n",
    "    model = accelerator.unwrap_model(model)\n",
    "    model.save_pretrained(f\"models/transformer_v2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e5787b9-0910-43d9-afe3-e7e554b58979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "num_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4b63d96-1cf7-4322-b553-84cd12d3348f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 8 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7662c771f1b4687bab193d05b17d659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/810 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db27fe2ca5264725ae6c5f9b8295afa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/810 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801a407f937b459da415e49f0c86b8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/810 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4105eeff30524beaaf1b8fc352f4f561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/810 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a84c3a613b84421821c6dddc0241931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/810 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b50ad2a400a74b1981abce02bdbb48b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/810 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7398d9883c3548e59dd17c1d692d9d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/810 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c581440918194b6f97369b0ce03f8dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/810 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2aa2276b66249c7ac6b29fad7b4ee4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12abf2879bcf401abdea3c7bfc7e69bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647b9b5a90ab4784861670e342504114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f216a64527e14a47ac9de4d97ca01544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2e869ae50c40bbabfb1cea96bf0b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa195091ac1482980418dbf183c3279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0eab1ecb5464548a762fa58366457b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b560d1a3cc5042e2836f4f892d25244d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fde165abf1f4a3f8f07e95757dcf093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75939 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c0a38fb1f2460aa36c1dcd5c308759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75939 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d295c8ad3c844b26b88443814b02b95a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75939 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d16b10c21f704c159647899322e28d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75939 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80927173e5ca4636b078f8ca28704083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75939 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0229896357414da881e3d47d406119ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75939 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5391dc67ed04ec59e497eb5c80fd957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75939 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2d16fde22749f49a40a2ca6a47eb66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75939 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 accuracy: 64.95%\n",
      "Epoch 2 accuracy: 65.88%\n",
      "Epoch 3 accuracy: 66.04%\n"
     ]
    }
   ],
   "source": [
    "args = (32, 5e-5, 3, \"fp16\", 0)\n",
    "notebook_launcher(training_loop, args, num_processes=num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06455858-3bd9-4178-a7ae-931e2bb310e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4779862-5fd7-473c-beeb-fdac5fed2bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
