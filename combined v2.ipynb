{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb0cdfb-7baf-429b-ba31-f41bc5f0536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.functional import cross_entropy\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from accelerate.utils import set_seed\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, \\\n",
    "    TrainingArguments, Trainer, get_scheduler, PretrainedConfig, PreTrainedModel\n",
    "\n",
    "np.random.seed(0) \n",
    "torch.manual_seed(0)\n",
    "\n",
    "#device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c69ab5-37b2-4041-8a9f-1b187efaef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/goodreads_train.csv\")\n",
    "train = train.sample(1000)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdfbb5e-0fca-4a34-9033-cef028529c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the votes and comments are not reliable \n",
    "train.loc[train[\"book_id\"]==18245960, ].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ba696b-d150-47c9-9926-3d7b4cd3f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train.loc[:, [\"user_id\", \"book_id\", \"review_text\", \"rating\"]]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6b34a5-3961-4340-ad5a-da526d40c1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode user and book ids\n",
    "enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                   unknown_value=-1)\n",
    "encodings = enc.fit_transform(data[[\"user_id\", \"book_id\"]])\n",
    "\n",
    "# add 1 to X_train, so unknowns would be 0\n",
    "encodings = encodings + 1\n",
    "\n",
    "encodings[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc7d22-2739-40fb-96ac-d59ef738b599",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"user_id\"] = encodings[:, 0].astype(int)\n",
    "data[\"book_id\"] = encodings[:, 1].astype(int)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29b08c9-e2bb-4921-88b7-71bd898eaab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users, n_books = data[\"user_id\"].max() + 1, data[\"book_id\"].max() + 1\n",
    "print(n_users, n_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a8a046-707c-417d-8f7d-bfde1296e23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a validation set\n",
    "train_df, valid_df = train_test_split(data, test_size=0.1, random_state=0)\n",
    "print(train_df.shape, valid_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cb6c34-94d0-48fb-8ee5-4a139f2a8f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model_name = \"distilbert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2138cd83-9ef6-4d51-aefc-8cb4823ae04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(batch_size):\n",
    "    train_ds = Dataset.from_pandas(train_df)\n",
    "    valid_ds = Dataset.from_pandas(valid_df)\n",
    "    \n",
    "    dataset = DatasetDict()\n",
    "    dataset[\"train\"] = train_ds\n",
    "    dataset[\"valid\"] = valid_ds\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(transformer_model_name)\n",
    "    tokenizer.save_pretrained(\"models/tokenizer/\")\n",
    "    \n",
    "    def tokenize_func(dataset, col=\"review_text\"):\n",
    "        return tokenizer(dataset[col], padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    tokenized_datasets = dataset.map(tokenize_func, batched=True)\n",
    "    \n",
    "    tokenized_datasets = tokenized_datasets.remove_columns(\n",
    "        [\"review_text\", \"__index_level_0__\"])\n",
    "    tokenized_datasets = tokenized_datasets.rename_column(\"rating\", \"labels\")\n",
    "    tokenized_datasets.set_format(\"torch\")\n",
    "    \n",
    "    train_dl = DataLoader(tokenized_datasets[\"train\"],\n",
    "                      shuffle=True,\n",
    "                      batch_size=batch_size)\n",
    "    valid_dl = DataLoader(tokenized_datasets[\"valid\"],\n",
    "                          batch_size=batch_size)\n",
    "    return train_dl, valid_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7d0063-4457-48c5-8f1c-638a9805b504",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, valid_dl = get_dataloaders(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec45977-c58e-42b1-a3d1-a4b5a10bbc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dl))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68eb6fd-e5e7-4e9a-b8f3-9b6d989bacbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedConfig(PretrainedConfig):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b90d06a-9ef8-4f90-8098-9c6ced777903",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(PreTrainedModel):\n",
    "    def __init__(self, config, n_users, n_books, transformer_model_name,\n",
    "                 n_factors=50, n_classes=6):\n",
    "        super().__init__(config)\n",
    "        self.user_embs = nn.Embedding(n_users, n_factors)\n",
    "        self.book_embs = nn.Embedding(n_books, n_factors)\n",
    "        self.text_transformer = AutoModelForSequenceClassification.from_pretrained(\n",
    "            transformer_model_name, num_labels=512)\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(n_factors*2+512, 256, bias=False),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, user_id=None, book_id=None,\n",
    "                input_ids=None, attention_mask=None, labels=None):\n",
    "        # first col: users, second col: books\n",
    "        x_users = self.user_embs(user_id)\n",
    "        x_books = self.book_embs(book_id)\n",
    "        x_transformer = self.text_transformer(input_ids=input_ids,\n",
    "                                              attention_mask=attention_mask).logits\n",
    "        x = torch.cat([x_users, x_books, x_transformer], dim=-1)\n",
    "        logits =  self.linear_layers(x)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss = cross_entropy(logits, labels)\n",
    "            return {\"loss\": loss, \"logits\": logits}\n",
    "        \n",
    "        return {\"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7df7cb1-1583-45d5-921f-910096b14d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    config = CombinedConfig()\n",
    "    model = CombinedModel(config, n_users, n_books, transformer_model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f185b7-39eb-43d8-9be1-f8665eac6e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(batch_size=32, lr=5e-5, num_epochs=3,\n",
    "                  mixed_precision=\"fp16\", seed=0):\n",
    "    set_seed(seed)\n",
    "    accelerator = Accelerator(mixed_precision=mixed_precision)\n",
    "    model = get_model()\n",
    "    #model.to(device)\n",
    "    train_dl, valid_dl = get_dataloaders(batch_size)\n",
    "    \n",
    "    num_training_steps = num_epochs * len(train_dl)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps)\n",
    "    \n",
    "    model, optimizer, train_dl, valid_dl, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dl, valid_dl, lr_scheduler)\n",
    "    \n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch in train_dl:\n",
    "            #batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs[\"loss\"]\n",
    "            #loss.backward()\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        model.eval()\n",
    "        accurate = 0\n",
    "        num_elems = 0\n",
    "        for batch in valid_dl:\n",
    "            #batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            logits = outputs[\"logits\"]\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            accurate_preds = accelerator.gather(predictions) == accelerator.gather(batch[\"labels\"])\n",
    "            #accurate_preds = predictions == batch[\"labels\"]\n",
    "            num_elems += accurate_preds.shape[0]\n",
    "            accurate += accurate_preds.long().sum()\n",
    "            \n",
    "        accuracy = accurate.item() / num_elems\n",
    "        accelerator.print(f\"Epoch {epoch+1} accuracy: {100*accuracy:.2f}%\")\n",
    "        #print(f\"Epoch {epoch+1} accuracy: {100*accuracy:.2f}%\")\n",
    "        \n",
    "    # save model\n",
    "    accelerator.wait_for_everyone()\n",
    "    model = accelerator.unwrap_model(model)\n",
    "    model.save_pretrained(f\"models/combined_v2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24a2f69-1a95-4bb8-b05a-787f6e31d65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_loop(batch_size=8, num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5787b9-0910-43d9-afe3-e7e554b58979",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "num_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b63d96-1cf7-4322-b553-84cd12d3348f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = (32, 5e-5, 10, \"fp16\", 0)\n",
    "notebook_launcher(training_loop, args, num_processes=num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e526c3d-2247-4c60-9164-86882adee6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
